{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract webtext from charter school data\n",
    "\n",
    "- Author: Jaren Haber\n",
    "- Institution: UC Berkeley\n",
    "- Date created: July 26, 2019\n",
    "- Date last edited: July 26, 2019\n",
    "\n",
    "Description: This simple notebook reads the charter data into memory, and saves only the WEBTEXT and NCESSCH columns to disk as a tab-separated file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd # For working with DataFrames\n",
    "import gc # To accelerate loading pickle files\n",
    "import ast # for recognizing strings (which Python thinks are floats)\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Show visualizations within notebook:\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions from data_tools directory:\n",
    "import sys; sys.path.insert(0, \"../tools\")\n",
    "\n",
    "# For displaying basic DF info, storing DFs for memory efficiency, and loading a filtered DF:\n",
    "from df_tools import check_df, convert_df, load_filtered_df, replace_df_nulls\n",
    "\n",
    "# For quickly loading & saving pickle files in Python:\n",
    "from quickpickle import quickpickle_dump, quickpickle_load \n",
    "\n",
    "# For saving and loading text lists to/from file:\n",
    "from textlist_file import write_list, load_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "unlapped_full_path = \"../../nowdata/charters_2015.pkl\"\n",
    "unlapped_filt10_path = \"../../nowdata/parsing/filtered_10.pkl\"\n",
    "original_full_path = \"../../sc_data/new_processed_df_070618.pkl\"\n",
    "original_filt250_path = \"../../nowdata/backups/charters_full_2015_250_v2a_orgtext.pkl\"\n",
    "raw_folder = \"../../nowdata/webtext_raw/\" # for raw webtext when extracted\n",
    "cleaned_folder = \"../../nowdata/webtext_cleaned/\" # for cleaned webtext: save as CSV, include ONLY the columns \"NCESSCH\" (unique school identifier) and \"text_full\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract webtext from filtered, overlap-removed (current) charter website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows and cols:  (10965, 2)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "NCESSCH\n",
      "WEBTEXT\n",
      "# rows and cols:  (6103, 2)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "text_full\n",
      "NCESSCH\n"
     ]
    }
   ],
   "source": [
    "# Load and filter WEBTEXT and NCESSCH\n",
    "df = load_filtered_df(\n",
    "    unlapped_filt10_path, \n",
    "    [\"WEBTEXT\", \"NCESSCH\"])\n",
    "\n",
    "# Filter out those with no webtext\n",
    "df = df[df[\"WEBTEXT\"].notnull()][df[\"WEBTEXT\"]!=0][df['WEBTEXT'].apply(len) > 0]\n",
    "\n",
    "# Filter out those with zero words\n",
    "df[\"NUMWORDS\"] = df[\"WEBTEXT\"].apply(\n",
    "    lambda x: sum([len(word_tokenize(page)) for page in x]))\n",
    "df = df[df['NUMWORDS'] > 0.0] \n",
    "\n",
    "# Remove NUMWORDS, rename webtext column\n",
    "df = df[[\"WEBTEXT\", \"NCESSCH\"]].rename(columns = {\"WEBTEXT\" : \"text_full\"}).reset_index(drop=True)\n",
    "check_df(df, \"NCESSCH\")\n",
    "\n",
    "# Dump filtered data to disk\n",
    "df.to_csv(raw_folder + \"webtext_unlapped_filtered_10.tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "#quickpickle_dump(df, raw_folder + \"webtext_filtered_10.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract webtext from UN-filtered, overlap-removed charter website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows and cols:  (10965, 2)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "NCESSCH\n",
      "WEBTEXT\n",
      "# rows and cols:  (6462, 2)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "text_full\n",
      "NCESSCH\n"
     ]
    }
   ],
   "source": [
    "# Load and filter WEBTEXT and NCESSCH\n",
    "df = load_filtered_df(\n",
    "    unlapped_full_path, \n",
    "    [\"WEBTEXT\", \"NCESSCH\"])\n",
    "\n",
    "# Filter out those with no webtext\n",
    "df = df[df[\"WEBTEXT\"].notnull()][df[\"WEBTEXT\"]!=0][df['WEBTEXT'].apply(len) > 0]\n",
    "\n",
    "# Filter out those with zero words\n",
    "df[\"NUMWORDS\"] = df[\"WEBTEXT\"].apply(\n",
    "    lambda x: sum([len(word_tokenize(tup[3])) for tup in x if tup!=[]]))\n",
    "df = df[df['NUMWORDS'] > 0.0] \n",
    "\n",
    "# Remove NUMWORDS, rename webtext column\n",
    "df = df[[\"WEBTEXT\", \"NCESSCH\"]].rename(columns = {\"WEBTEXT\" : \"text_full\"}).reset_index(drop=True)\n",
    "check_df(df, \"NCESSCH\")\n",
    "\n",
    "# Dump filtered data to disk\n",
    "df.to_csv(raw_folder + \"webtext_unlapped_full.tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "#quickpickle_dump(df, raw_folder + \"webtext_full_unlapped.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract webtext from UN-filtered, UN-overlap-removed (original) charter website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows and cols:  (6504, 2)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "text_full\n",
      "NCESSCH\n"
     ]
    }
   ],
   "source": [
    "# Load and filter WEBTEXT and NCESSCH\n",
    "df = pd.read_pickle(original_full_path)\n",
    "df = df[[\"data\", \"NCESSCH\"]]\n",
    "\n",
    "# For consistency, rename column:\n",
    "df.rename(inplace = True, columns = {\"data\" : \"WEBTEXT\"})\n",
    "\n",
    "# Filter out those with no webtext\n",
    "df = df[df[\"WEBTEXT\"].notnull()][df[\"WEBTEXT\"]!=0][df['WEBTEXT'].apply(len) > 0]\n",
    "\n",
    "# Filter out those with zero words\n",
    "df[\"NUMWORDS\"] = df[\"WEBTEXT\"].apply(\n",
    "    lambda x: sum([len(word_tokenize(tup[3])) for tup in x if tup!=[]]))\n",
    "df = df[df['NUMWORDS'] > 0.0] \n",
    "\n",
    "# Remove NUMWORDS, rename webtext column\n",
    "df = df[[\"WEBTEXT\", \"NCESSCH\"]].rename(columns = {\"WEBTEXT\" : \"text_full\"}).reset_index(drop=True)\n",
    "check_df(df, \"NCESSCH\")\n",
    "\n",
    "# Dump filtered data to disk\n",
    "df.to_csv(raw_folder + \"webtext_original_full.tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "#quickpickle_dump(df, raw_folder + \"webtext_full_original.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract webtext from UN-filtered, UN-overlap-removed, filtered 250 charter website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows and cols:  (6463, 2)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "text_full\n",
      "NCESSCH\n"
     ]
    }
   ],
   "source": [
    "# Load and filter WEBTEXT and NCESSCH\n",
    "df = load_filtered_df(original_filt250_path, [\"WEBTEXT\", \"NCESSCH\"])\n",
    "\n",
    "# Coerce text column from string to list of strings, while extracting page text only (from list of quadruples)\n",
    "#df['WEBTEXT'] = df['WEBTEXT'].apply(lambda x: [ast.literal_eval(page[3]) for page in x])\n",
    "\n",
    "# Filter out those with no webtext\n",
    "df = df[df[\"WEBTEXT\"].notnull()][df[\"WEBTEXT\"]!=0][df['WEBTEXT'].astype(str).apply(len) > 0]\n",
    "\n",
    "# Filter out those with zero words (count words first)\n",
    "df[\"NUMWORDS\"] = df[\"WEBTEXT\"].apply(\n",
    "    lambda x: sum([len(word_tokenize(tup[3])) for tup in x if tup!=[]]))\n",
    "df = df[df['NUMWORDS'] > 0.0] \n",
    "\n",
    "# Remove NUMWORDS, rename webtext column\n",
    "df = df[[\"WEBTEXT\", \"NCESSCH\"]].rename(columns = {\"WEBTEXT\" : \"text_full\"}).reset_index(drop=True)\n",
    "check_df(df, \"NCESSCH\")\n",
    "\n",
    "# Dump filtered data to disk\n",
    "df.to_csv(raw_folder + \"webtext_original_filtered_250.tsv\", sep = \"\\t\", encoding = \"utf-8\")\n",
    "#quickpickle_dump(df, raw_folder + \"webtext_full_original.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
