{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean webtext from charter school data\n",
    "\n",
    "- Author: Jaren Haber, Madeleine Peng, James Jung\n",
    "- Institution: UC Berkeley\n",
    "- Date created: July 26, 2019\n",
    "- Date last edited: \n",
    "\n",
    "Description: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd # For working with DataFrames\n",
    "import gc # To accelerate loading pickle files\n",
    "\n",
    "# Show visualizations within notebook:\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions from data_tools directory:\n",
    "import sys; sys.path.insert(0, \"../tools\")\n",
    "\n",
    "# For displaying basic DF info, storing DFs for memory efficiency, and loading a filtered DF:\n",
    "from df_tools import check_df, convert_df, load_filtered_df, replace_df_nulls\n",
    "\n",
    "# For quickly loading & saving pickle files in Python:\n",
    "from quickpickle import quickpickle_dump, quickpickle_load \n",
    "\n",
    "# For saving and loading text lists to/from file:\n",
    "from textlist_file import write_list, load_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "charters_path = \"../../nowdata/charters_2015.pkl\"\n",
    "original_path = \"../../nowdata/backups/charters_full_2015_250_v2a_unlappedtext_counts3_geoleaid.pkl\"\n",
    "filtered_path = \"../../nowdata/parsing/filtered_10.pkl\"\n",
    "raw_folder = \"../../nowdata/webtext_raw/\" # for raw webtext when extracted\n",
    "raw_filtered_data = raw_folder + \"webtext_unlapped_filtered_10.pkl\"\n",
    "cleaned_folder = \"../../nowdata/webtext_cleaned/\" # for cleaned webtext: save as CSV, include ONLY the columns \"NCESSCH\" (unique school identifier) and \"text_full\" (renamed from \"WEBTEXT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper function(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_webtext(ls):\n",
    "    \n",
    "    '''This function cleans and tokenizes sentences, removing punctuation and numbers and making words into lower-case stems.\n",
    "    Inputs: list of strings;\n",
    "    This function loops over all elements in the input list given, cleans the texts and returns one string'''\n",
    "        \n",
    "    global mpdo # Check if we're doing multiprocessing. If so, then mpdo=True\n",
    "    global sents_combined # Grants access to variable holding a list of lists of words, where each list of words represents a sentence in its original order (only relevant for this function if we're not using multiprocessing)\n",
    "    global pcount # Grants access to preprocessing counter\n",
    "    \n",
    "    known_pages = set() # Initialize list of known pages for a school\n",
    "    sents_combined = [] # Initialize list of all school's sentences\n",
    "    \n",
    "    #print('Parsing school #' + str(pcount)) # Print number of school being parsed\n",
    "\n",
    "    for s in ls: # Iterate over tuples in tuplist (list of tuples)\n",
    "        for chunk in s.split('\\n'): \n",
    "            for sent in sent_tokenize(chunk): # Tokenize chunk by sentences (in case >1 sentence in chunk)\n",
    "                #sent = clean_sentence(sent, fast=True) # Clean and tokenize sentence\n",
    "                sent = clean_sentence(sent)\n",
    "                if ((sent == []) or (len(sent) == 0)): # If sentence is empty, continue to next sentence without appending\n",
    "                    continue\n",
    "                \n",
    "                sents_combined.extend(sent) # add sent to school object\n",
    "                \n",
    "    school_sentslist.append(sents_combined) # add list of sentence to full list \n",
    "    \n",
    "    return sents_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows and cols:  (6103, 2)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "text_full\n",
      "NCESSCH\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = quickpickle_load(raw_filtered_data)[[\"text_full\", \"NCESSCH\"]]\n",
    "check_df(df, \"NCESSCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_str = [] #initialize list for concatenated string for webtext per school\n",
    "\n",
    "for i in range(len(df)):\n",
    "    ls_str.append(' '.join(preprocess_wem2(df['WEBTEXT'][i])))\n",
    "    if i%500 == 0:\n",
    "        print(\"Cleaned \", i, \" rows\")\n",
    "    \n",
    "df['text_full'] = ls_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next step: Merging!\n",
    "Now merge this cleaned text with the covariates from the previous version of the charter schools data (change only the `text_full` column)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
