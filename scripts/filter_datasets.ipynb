{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter public/charter school data into team-specific data sets\n",
    "\n",
    "Author(s): Jaren Haber<br>\n",
    "Project Manager: Jaren Haber, PhD Candidate <br>\n",
    "Contact: jhaber@berkeley.edu\n",
    "\n",
    "Institution: University of California, Berkeley <br>\n",
    "Program: Undergraduate Research Apprentice Program (URAP) <br>\n",
    "\n",
    "Date created: Nov. 27, 2018<br>\n",
    "Last modified: Nov. 27, 2018\n",
    "\n",
    "Description: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # For working with DataFrames\n",
    "import gc # For speeding up loading pickle files ('gc' = 'garbage collector')\n",
    "import ast # For working with strings\n",
    "import numpy as np # For numerical things\n",
    "import re # For cleaning webtext\n",
    "import _pickle as cPickle # Optimized version of pickle\n",
    "import gc # For managing garbage collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files:\n",
    "charters_path = \"../../nowdata/charters_2015.pkl\"\n",
    "pubschools_path = \"../../nowdata/pubschools_2015.pkl\"\n",
    "\n",
    "charters_small_loc = \"../../nowdata/backups/charters_parsed_03-04_no-text_SMALL.csv\"\n",
    "ACSsmall_loc = \"../data/ACS_2016_sd-merged_SMALL.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output files:\n",
    "charters_storepath = \"../../nowdata/backups/charters_full_2015_250_v2a.pkl\"\n",
    "pubschools_storepath = \"../../nowdata/backups/pubschools_full_2015_CRDC.pkl\"\n",
    "\n",
    "geo_storepath = \"../../nowdata/backups/charters_geo_2015_v2a.csv\"\n",
    "stats_storepath = \"../../nowdata/backups/charters_stats_2015_v2a.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df(df, ignore_list):\n",
    "    \"\"\"Makes a Pandas DataFrame more memory-efficient through intelligent use of Pandas data types: \n",
    "    specifically, by storing columns with repetitive Python strings not with the object dtype for unique values \n",
    "    (entirely stored in memory) but as categoricals, which are represented by repeated integer values. This is a \n",
    "    net gain in memory when the reduced memory size of the category type outweighs the added memory cost of storing \n",
    "    one more thing. As such, this function checks the degree of redundancy for a given column before converting it.\"\"\"\n",
    "    \n",
    "    # Remove specified columns to avoid conversion errors, those that shouldn't have their dtype converted\n",
    "    # e.g., columns that are large lists of tuples, like \"WEBTEXT\" or \"CMO_WEBTEXT\", should stay as 'object' dtype\n",
    "    if len(ignore_list)>0:\n",
    "        ignore_df = df[ignore_list]\n",
    "        df.drop(ignore_list, axis=1, inplace=True)\n",
    "    \n",
    "    converted_df = pd.DataFrame() # Initialize DF for memory-efficient storage of strings (object types)\n",
    "    df_obj = df.select_dtypes(include=['object']).copy() # Filter to only those columns of object data type\n",
    "\n",
    "    # Loop through all columns that have 'object' dtype, b/c we especially want to convert these if possible:\n",
    "    for col in df.columns: \n",
    "        if col in df_obj: \n",
    "            num_unique_values = len(df_obj[col].unique())\n",
    "            num_total_values = len(df_obj[col])\n",
    "            if (num_unique_values / num_total_values) < 0.5: # Only convert data types if at least half of values are duplicates\n",
    "                converted_df.loc[:,col] = df[col].astype('category') # Store these columns as dtype \"category\"\n",
    "            else: \n",
    "                converted_df.loc[:,col] = df[col]\n",
    "        else:    \n",
    "            converted_df.loc[:,col] = df[col]\n",
    "                      \n",
    "    # Downcast dtype to reduce memory drain\n",
    "    converted_df.select_dtypes(include=['float']).apply(pd.to_numeric,downcast='float')\n",
    "    converted_df.select_dtypes(include=['int']).apply(pd.to_numeric,downcast='signed')\n",
    "    \n",
    "    # Reintroduce ignored columns into resulting DF\n",
    "    if len(ignore_list)>0:\n",
    "        for col in ignore_list:\n",
    "            converted_df[col] = ignore_df[col]\n",
    "    \n",
    "    return converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickpickle_load(picklepath):\n",
    "    '''Very time-efficient way to load pickle-formatted objects into Python.\n",
    "    Uses C-based pickle (cPickle) and gc workarounds to facilitate speed. \n",
    "    Input: Filepath to pickled (*.pkl) object.\n",
    "    Output: Python object (probably a list of sentences or something similar).'''\n",
    "\n",
    "    with open(picklepath, 'rb') as loadfile:\n",
    "        \n",
    "        gc.disable() # disable garbage collector\n",
    "        outputvar = cPickle.load(loadfile) # Load from picklepath into outputvar\n",
    "        gc.enable() # enable garbage collector again\n",
    "    \n",
    "    return outputvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickpickle_dump(dumpvar, picklepath):\n",
    "    '''Very time-efficient way to dump pickle-formatted objects from Python.\n",
    "    Uses C-based pickle (cPickle) and gc workarounds to facilitate speed. \n",
    "    Input: Python object (probably a list of sentences or something similar).\n",
    "    Output: Filepath to pickled (*.pkl) object.'''\n",
    "\n",
    "    with open(picklepath, 'wb') as destfile:\n",
    "        \n",
    "        gc.disable() # disable garbage collector\n",
    "        cPickle.dump(dumpvar, destfile) # Dump dumpvar to picklepath\n",
    "        gc.enable() # enable garbage collector again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df(DF, colname):\n",
    "    \"\"\"Displays basic info about a dataframe in memory.\n",
    "    Input: Pandas DataFrame object\n",
    "    Output: printed basic stats:    # rows and columns, \n",
    "                                    # duplicates by colname, \n",
    "                                    column names and, if missing data, the # missing cases.\"\"\"\n",
    "    \n",
    "    # Show DF info, including # duplicates by colname\n",
    "    print(\"# rows and cols: \", str(DF.shape))\n",
    "    print(\"# duplicates by \" + str(colname) + \": \" + str(sum(DF.duplicated(subset=colname, keep='first'))))\n",
    "\n",
    "    print(\"\\nColumns and # missing cases (if any): \")\n",
    "    for col in list(DF):\n",
    "        missed = sum(DF[colname].isnull())\n",
    "        if missed > 0:\n",
    "            print(col + \": \" + str(missed) + \" missing\")\n",
    "        else:\n",
    "            print(col)\n",
    "    \n",
    "    #print(\"\\nALL column names: \", list(DF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filtered_df(dfpath, keepcols):\n",
    "    \"\"\"Quickly loads a Pandas DataFrame from file (either .csv or .pkl format), \n",
    "    keeps only those variables in keepvars (if not an empty list), and makes the DF memory-efficient.\n",
    "    Input: file path to DataFrame (.csv or .pkl), list of variables to keep from said DF (or empty list, to keep all cols)\n",
    "    Output: DF with reduced variables and with memory-efficient dtypes.\"\"\"\n",
    "    \n",
    "    if len(keepcols)>0:\n",
    "        if dfpath.endswith(\".csv\"):\n",
    "            newdf = pd.read_csv(dfpath, usecols=keepcols, low_memory=False)\n",
    "        elif dfpath.endswith(\".pkl\"):\n",
    "            newdf = quickpickle_load(dfpath)\n",
    "            newdf = newdf[keepcols]\n",
    "            \n",
    "    else:\n",
    "        if dfpath.endswith(\".csv\"):\n",
    "            newdf = pd.read_csv(dfpath, low_memory=False)\n",
    "        elif dfpath.endswith(\".pkl\"):\n",
    "            newdf = quickpickle_load(dfpath)\n",
    "    \n",
    "    if \"WEBTEXT\" in list(newdf) or \"CMO_WEBTEXT\" in list(newdf):\n",
    "        newdf = convert_df(newdf, [\"WEBTEXT\", \"CMO_WEBTEXT\"])\n",
    "    else:\n",
    "        newdf = convert_df(newdf, [])\n",
    "    \n",
    "    if \"NCESSCH\" in list(newdf):\n",
    "        newdf[\"NCESSCH\"] = newdf[\"NCESSCH\"].astype(float)\n",
    "        check_df(newdf, \"NCESSCH\")\n",
    "    \n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_calc(somedf, largedf=None, groupvar, uniqueid):\n",
    "    \"\"\"Calculates total number of entities (rows) in a given DataFrame that share a given clustering/group variable.\n",
    "    Uses uniqueid to identify number of independent entities. Finally merges the density with the given DF.\n",
    "    Useful for calculating the density of charter/public schools in a given school district.\n",
    "    Input: DataFrame, variable to group by, unique IDs (for each entity), variable name for density.\n",
    "    Output: DataFrame with density variable added.\"\"\"\n",
    "\n",
    "    # Keep only relevant variables from somedf for finding density\n",
    "    grouped = somedf[[groupvar, uniqueid]]\n",
    "    \n",
    "    # TO DO: Use pubdf (as largedf) to calculate density of both public schools and charter schools in somedf\n",
    "    \n",
    "    # Generate 2-element DF grouped by groupvar, identifying distinct entities using uniqueid:\n",
    "    grouped = grouped.groupby([groupvar])[uniqueid].count().reset_index(name=\"Number_entities\")\n",
    "    \n",
    "    # Merge density column into original DF\n",
    "    densitycol = pd.merge(somedf, grouped, how='outer', on=[groupvar])[\"Number_entities\"]\n",
    "    \n",
    "    #new_frame[densityvar] = new_frame['All_school_counts']/merge_frame[(\"Area (Land)\", \"Geo_AREALAND\")]\n",
    "    \n",
    "    return densitycol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list(file_path, textlist):\n",
    "    \"\"\"Writes textlist to file_path.\n",
    "    Input: Path to file, list of strings\n",
    "    Output: Nothing (saved to disk)\"\"\"\n",
    "    \n",
    "    with open(file_path, 'w') as file_handler:\n",
    "        \n",
    "        for elem in textlist:\n",
    "            file_handler.write(\"{}\\n\".format(elem))\n",
    "    \n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list(file_path):\n",
    "    \"\"\"Loads list into memory. Must be assigned to object.\n",
    "    Input: Path to file\n",
    "    Output: List object\"\"\"\n",
    "    \n",
    "    textlist = []\n",
    "    with open(file_path) as file_handler:\n",
    "        line = file_handler.readline()\n",
    "        while line:\n",
    "            textlist.append(line)\n",
    "            line = file_handler.readline()\n",
    "    return textlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows and cols:  (6972, 32)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "STATENAME\n",
      "LEAID\n",
      "LEA_NAME\n",
      "NCESSCH\n",
      "SCH_NAME\n",
      "TOTFRL\n",
      "AM\n",
      "AS\n",
      "BL\n",
      "HI\n",
      "HP\n",
      "MEMBER\n",
      "TR\n",
      "TITLEI\n",
      "FTE\n",
      "YEAR_OPENED\n",
      "YEAR_CLOSED\n",
      "LATCODE\n",
      "LONGCODE\n",
      "LOCALE\n",
      "ALL_MTH00PCTPROF_1415\n",
      "ALL_RLA00PCTPROF_1415\n",
      "ADDRESS14\n",
      "ess_count\n",
      "prog_count\n",
      "rit_count\n",
      "ess_strength\n",
      "prog_strength\n",
      "AGE\n",
      "PCTETH\n",
      "PLACE\n",
      "TOTETH\n"
     ]
    }
   ],
   "source": [
    "charters_smalldf = load_filtered_df(charters_small_loc, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to keep from big data set\n",
    "keepvars = ['LEVEL', 'MEMBER', 'SE_T002_002', 'AGE', 'PCTETH', 'PCTFRL', 'PCTETH_SD', 'PCT_SE_T113_002', \n",
    "            'ESS_VALID_RATIO', 'PROG_VALID_RATIO', 'INQUIRY_RATIO', 'DISCIPLINE_RATIO', 'STABR', 'LEAID', 'GEO_LEAID',\n",
    "            'NCESSCH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows and cols:  (136825, 2)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "NCESSCH\n",
      "GEO_LEAID\n"
     ]
    }
   ],
   "source": [
    "# Load full public school data set - just the cols needed to count density\n",
    "pubdf_small = load_filtered_df(pubschools_path, [\"NCESSCH\", \"GEO_LEAID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows and cols:  (10965, 16)\n",
      "# duplicates by NCESSCH: 0\n",
      "\n",
      "Columns and # missing cases (if any): \n",
      "LEVEL\n",
      "MEMBER\n",
      "SE_T002_002\n",
      "AGE\n",
      "PCTETH\n",
      "PCTFRL\n",
      "PCTETH_SD\n",
      "PCT_SE_T113_002\n",
      "ESS_VALID_RATIO\n",
      "PROG_VALID_RATIO\n",
      "INQUIRY_RATIO\n",
      "DISCIPLINE_RATIO\n",
      "STABR\n",
      "LEAID\n",
      "GEO_LEAID\n",
      "NCESSCH\n"
     ]
    }
   ],
   "source": [
    "# Load and filter charter data set\n",
    "charterdf = load_filtered_df(charters_path, keepvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new variable with density of charter schools\n",
    "charterdf[\"CHARTER_DENSITY\"] = density_calc(charterdf, pubdf_small, \"GEO_LEAID\", \"NCESSCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop identifier (not needed for analysis)\n",
    "charterdf.drop(columns=\"NCESSCH\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Rename variables\n",
    "read_charterdf = charterdf.rename(\n",
    "    index=str, columns={'LEVEL':, 'MEMBER':'Student_count', 'SE_T002_002':, 'AGE':'School_age', \n",
    "                        'PCTETH':, 'PCTFRL':, 'PCTETH_SD':, 'PCT_SE_T113_002':, \n",
    "                        'ESS_VALID_RATIO':, 'PROG_VALID_RATIO':, 'INQUIRY_RATIO':'IBL_emphasis',\n",
    "                        'DISCIPLINE_RATIO':'Discipline_emphasis', 'STABR':'State', 'LEAID':'SD_formal', \n",
    "                        'GEO_LEAID':'SD_geog'})\n",
    "                        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to disk\n",
    "charterdf.to_csv(stats_storepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quickpickle_dump(charterdf, charters_storepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv(\"../../nowdata/schooldf_filtered_Nov18.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
